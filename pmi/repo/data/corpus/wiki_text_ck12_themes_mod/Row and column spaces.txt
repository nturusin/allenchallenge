In linear algebra, the column space C(A) of a matrix A (sometimes called the range of a matrix) is the set of all possible linear combinations of its column vectors. Let K be a field (such as real or complex numbers). The column space of an mn matrix with components from K is a linear subspace of the m-space Km. The dimension of the column space is called the rank of the matrix. A definition for matrices over a ring K (such as integers) is also possible. The column space of a matrix is the image or range of the corresponding matrix transformation. The row space and column space of an m-by-n matrix are the linear subspaces generated by row vectors and column vectors, respectively, of the matrix. Its dimension is equal to the rank of the matrix and is at most min(m,n). The rest of article will consider matrices of real numbers: row and column spaces are subspace of Rn and Rm real spaces respectively. But row and column spaces can be constructed from matrices with components in any field and even a ring. Let A be an m-by-n matrix. Then rank(A) = dim(rowsp(A)) = dim(colsp(A)), rank(A) = number of pivots in any echelon form of A, rank(A) = the maximum number of linearly independent rows or columns of A. If one considers the matrix as a linear transformation from Rn to Rm, then the column space of the matrix equals the image of this linear transformation. The column space of a matrix A is the set of all linear combinations of the columns in A. If A = [a1, ...., an], then colsp(A) = span {a1, ...., an}. The concept of row space generalises to matrices to C, the field of complex numbers, or to any field. Intuitively, given a matrix A, the action of the matrix A on a vector x will return a linear combination of the columns of A weighted by the coordinates of x as coefficients. Another way to look at this is that it will (1) first project x into the row space of A, (2) perform an invertible transformation, and (3) place the resulting vector y in the column space of A. Thus the result y =A x must reside in the column space of A. See the singular value decomposition for more details on this second interpretation. Given a matrix J: the rows are r1 = (2,4,1,3,2), r2 = (1,2,1,0,5), r3 = (1,6,2,2,2), r4 = (3,6,2,5,1). Consequently the row space of J is the subspace of R5 spanned by { r1, r2, r3, r4 }. Since these four row vectors are linearly independent, the row space is 4-dimensional. Moreover in this case it can be seen that they are all orthogonal to the vector n = (6,1,4,4,0), so it can be deduced that the row space consists of all vectors in R5 that are orthogonal to n. Let K be a field of scalars. Let A be an mn matrix, with column vectors v1, v2, ..., vn. A linear combination of these vectors is any vector of the form where c1, c2, ..., cn are scalars. The set of all possible linear combinations of v1,...,vn is called the column space of A. That is, the column space of A is the span of the vectors v1,...,vn. Any linear combination of the column vectors of a matrix A can be written as the product of A with a column vector: Therefore, the column space of A consists of all possible products Ax, for x Cn. This is the same as the image (or range) of the corresponding matrix transformation. Example If , then the column vectors are v1 = (1, 0, 2)T and v2 = (0, 1, 0)T. A linear combination of v1 and v2 is any vector of the form The set of all such vectors is the column space of A. In this case, the column space is precisely the set of vectors (x, y, z) R3 satisfying the equation z = 2x (using Cartesian coordinates, this set is a plane through the origin in three-dimensional space). The columns of A span the column space, but they may not form a basis if the column vectors are not linearly independent. Fortunately, elementary row operations do not affect the dependence relations between the column vectors. This makes it possible to use row reduction to find a basis for the column space. For example, consider the matrix The columns of this matrix span the column space, but they may not be linearly independent, in which case some subset of them will form a basis. To find this basis, we reduce A to reduced row echelon form: At this point, it is clear that the first, second, and fourth columns are linearly independent, while the third column is a linear combination of the first two. (Specifically, v3 = 2v1 + v2.) Therefore, the first, second, and fourth columns of the original matrix are a basis for the column space: Note that the independent columns of the reduced row echelon form are precisely the columns with pivots. This makes it possible to determine which columns are linearly independent by reducing only to echelon form. The above algorithm can be used in general to find the dependence relations between any set of vectors, and to pick out a basis from any spanning set. A different algorithm for finding a basis from a spanning set is given in the row space article; finding a basis for the column space of A is equivalent to finding a basis for the row space of the transpose matrix AT. The dimension of the column space is called the rank of the matrix. The rank is equal to the number of pivots in the reduced row echelon form, and is the maximum number of linearly independent columns that can be chosen from the matrix. For example, the 4 4 matrix in the example above has rank three. Because the column space is the image of the corresponding matrix transformation, the rank of a matrix is the same as the dimension of the image. For example, the transformation R4 R4 described by the matrix above maps all of R4 to some three-dimensional subspace. The nullity of a matrix is the dimension of the null space, and is equal to the number of columns in the reduced row echelon form that do not have pivots. The rank and nullity of a matrix A with n columns are related by the equation: This is known as the rank-nullity theorem. The left null space of A is the set of all vectors x such that xTA = 0T. It is the same as the null space of the transpose of A. The product of the matrix AT and the vector x can be written in terms of the dot product of vectors: because row vectors of AT are transposes of column vectors vk of A. Thus ATx = 0 if and only if x is orthogonal (perpendicular) to each of the column vectors of A. It follows that the left null space (the null space of AT) is the orthogonal complement to the column space of A. For a matrix A, the column space, row space, null space, and left null space are sometimes referred to as the four fundamental subspaces. Similarly the column space (sometimes disambiguated as right column space) can be defined for matrices over a ring K as for any c1, ..., cn, with replacement of the vector m-space with "right free module", which changes the order of scalar multiplication of the vector vk to the scalar ck such that it is written in an unusual order vectorscalar. Let K be a field of scalars. Let A be an mn matrix, with row vectors r1,r2,...,rm. A linear combination of these vectors is any vector of the form where c1,c2,...,cm are scalars. The set of all possible linear combinations of r1,...,rm is called the row space of A. That is, the row space of A is the span of the vectors r1,...,rm. For example, if then the row vectors are r1 = (1,0,2) and r2 = (0,1,0). A linear combination of r1 and r2 is any vector of the form The set of all such vectors is the row space of A. In this case, the row space is precisely the set of vectors (x,y,z) K3 satisfying the equation z = 2x (using Cartesian coordinates, this set is a plane through the origin in three-dimensional space). For a matrix that represents a homogeneous system of linear equations, the row space consists of all linear equations that follow from those in the system. The column space of A is equal to the row space of AT. The row space is not affected by elementary row operations. This makes it possible to use row reduction to find a basis for the row space. For example, consider the matrix The rows of this matrix span the row space, but they may not be linearly independent, in which case the rows will not be a basis. To find a basis, we reduce A to row echelon form: r1, r2, r3 represents the rows. Once the matrix is in echelon form, the nonzero rows are a basis for the row space. In this case, the basis is { (1,3,2), (0,1,0) }. Another possible basis { (1,0,2), (0,1,0) } comes from a further reduction. This algorithm can be used in general to find a basis for the span of a set of vectors. If the matrix is further simplified to reduced row echelon form, then the resulting basis is uniquely determined by the row space. The dimension of the row space is called the rank of the matrix. This is the same as the maximum number of linearly independent rows that can be chosen from the matrix, or equivalently the number of pivots. For example, the 33 matrix in the example above has rank two. The rank of a matrix is also equal to the dimension of the column space. The dimension of the null space is called the nullity of the matrix, and is related to the rank by the following equation: where n is the number of columns of the matrix A. The equation above is known as the rank-nullity theorem. The null space of matrix A is the set of all vectors x for which Ax = 0. The product of the matrix A and the vector x can be written in terms of the dot product of vectors: where r1,...,rm are the row vectors of A. Thus Ax = 0 if and only if x is orthogonal (perpendicular) to each of the row vectors of A. It follows that the null space of A is the orthogonal complement to the row space. For example, if the row space is a plane through the origin in three dimensions, then the null space will be the perpendicular line through the origin. This provides a proof of the rank-nullity theorem (see dimension above). The row space and null space are two of the four fundamental subspaces associated with a matrix A (the other two being the column space and left null space). If V and W are vector spaces, then the kernel of a linear transformation T: V W is the set of vectors v V for which T(v) = 0. The kernel of a linear transformation is analogous to the null space of a matrix. If V is an inner product space, then the orthogonal complement to the kernel can be thought of as a generalization of the row space. This is sometimes called the coimage of T. The transformation T is one-to-one on its coimage, and the coimage maps isomorphically onto the image of T. When V is not an inner product space, the coimage of T can be defined as the quotient space V / ker(T). 