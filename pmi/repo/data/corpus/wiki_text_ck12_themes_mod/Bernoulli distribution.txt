In probability theory and statistics, the Bernoulli distribution, named after Swiss scientist Jacob Bernoulli, is the probability distribution of a random variable which takes the value 1 with success probability of and the value 0 with failure probability of . It can be used to represent a coin toss where 1 and 0 would represent "head" and "tail" (or vice versa), respectively. In particular, unfair coins would have . The Bernoulli distribution is a special case of the two-point distribution, for which the two possible outcomes need not be 0 and 1. If is a random variable with this distribution, we have: The probability mass function of this distribution, over possible outcomes k, is This can also be expressed as The expected value of a Bernoulli random variable is and its variance is The Bernoulli distribution is a special case of the binomial distribution with . The kurtosis goes to infinity for high and low values of , but for the two-point distributions including the Bernoulli distribution have a lower excess kurtosis than any other probability distribution, namely 2. The Bernoulli distributions for form an exponential family. The maximum likelihood estimator of based on a random sample is the sample mean. If are independent, identically distributed (i.i.d.) random variables, all Bernoulli distributed with success probability p, then (binomial distribution). The Bernoulli distribution is simply . The categorical distribution is the generalization of the Bernoulli distribution for variables with any constant number of discrete values. The Beta distribution is the conjugate prior of the Bernoulli distribution. The geometric distribution models the number of independent and identical Bernoulli trials needed to get one success. If Y ~ Bernoulli(0.5), then (2Y-1) has a Rademacher distribution. 